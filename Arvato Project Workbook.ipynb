{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "from webencodings import labels\n",
    "\n",
    "\n",
    "# load in the data\n",
    "azdias = pd.read_csv('./data/Udacity_AZDIAS_052018.csv')[:10000]\n",
    "customers = pd.read_csv('./data/Udacity_CUSTOMERS_052018.csv')[:10000]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "azdias.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "customers.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from pandas.core.arrays.categorical import factorize_from_iterable\n",
    "import pickle\n",
    "\n",
    "\n",
    "class OpDataFrame(ABC):\n",
    "    def __init__(self, op_name, ignore_if_failure=False):\n",
    "        self.op_name = op_name\n",
    "        self.ignore_if_failure = ignore_if_failure\n",
    "\n",
    "    @abstractmethod\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        return _df\n",
    "\n",
    "    def get_info(self):\n",
    "        if hasattr(self, \"field\"):\n",
    "            return f\"{self.op_name}: {self.field}\"\n",
    "        return self.op_name\n",
    "\n",
    "    # def __eq__(self, obj):\n",
    "    #     return isinstance(obj, OpDataFrame) and (obj.op_name == self.op_name) and hasattr(obj, \"field\") and hasattr(self, \"field\") and (obj.field == self.field)\n",
    "\n",
    "\n",
    "class OpConvertDate(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Get datetime values from the date fields from data frame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field):\n",
    "        super(OpConvertDate, self).__init__(op_name=\"OpConvertDate\")\n",
    "        self.field = field\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = pd.to_datetime(_df[self.field])\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpConvertBoolean(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Convert columns `field` in dataframe `_df` into boolean value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field, t_value='t'):\n",
    "        super(OpConvertBoolean, self).__init__(op_name=\"OpConvertBoolean\")\n",
    "        self.field = field\n",
    "        self.t_value = t_value\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].astype(str) == self.t_value\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpConvertMoneyToFloat(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Convert money with sign $ to float value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field):\n",
    "        super(OpConvertMoneyToFloat, self).__init__(op_name=\"OpConvertMoneyToFloat\")\n",
    "        self.field = field\n",
    "        # self.money_sign = money_sign\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpConvertToFloat(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Convert value to float\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field):\n",
    "        super(OpConvertToFloat, self).__init__(op_name=\"OpConvertToFloat\")\n",
    "        self.field = field\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].astype(float)\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpConvertPercentToFloat(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Convert percent values to float (Ex 80% = 0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field):\n",
    "        super(OpConvertPercentToFloat, self).__init__(op_name=\"OpConvertPercentToFloat\")\n",
    "        self.field = field\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].str.rstrip('%').astype('float') / 100.0\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpConvertToString(OpDataFrame):\n",
    "    def __init__(self, field):\n",
    "        super(OpConvertToString, self).__init__(op_name=\"OpConvertToString\")\n",
    "        self.field = field\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].astype('string')\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpDropColumns(OpDataFrame):\n",
    "    def __init__(self, fields):\n",
    "        super(OpDropColumns, self).__init__(op_name=\"OpDropColumns\")\n",
    "        self.fields = fields\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _column_deleting = list(set(self.fields) & set(_df.columns))\n",
    "        _df = _df.drop(labels=_column_deleting, axis=1)\n",
    "\n",
    "        return _df\n",
    "\n",
    "\n",
    "class CanSave(object):\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def save(self, data):\n",
    "        if self.save_path:\n",
    "            with open(self.save_path, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "class CanLoad(object):\n",
    "    def __init__(self, load_path):\n",
    "        self.load_path = load_path\n",
    "\n",
    "    def load(self):\n",
    "        if self.load_path:\n",
    "            with open(self.load_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "\n",
    "\n",
    "class CategoryDummy(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_index(self, labels, code_name):\n",
    "        def _get_nan_index():\n",
    "            for i, v in enumerate(labels):\n",
    "                try:\n",
    "                    if isinstance(v,(float, int)) and np.isnan(v):\n",
    "                        return i\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        if isinstance(code_name, (float, int)):\n",
    "            if np.isnan(code_name):\n",
    "                return _get_nan_index()\n",
    "\n",
    "        for i, v in enumerate(labels):\n",
    "            if v == code_name:\n",
    "                return i\n",
    "\n",
    "        return _get_nan_index()\n",
    "\n",
    "    def codes_labels(self, series: pd.Series):\n",
    "        codes, labels = factorize_from_iterable(series)\n",
    "\n",
    "        codes = codes.copy()\n",
    "        # Dummy na is default\n",
    "        codes[codes == -1] = len(labels)\n",
    "        levels = np.append(labels, np.nan)\n",
    "\n",
    "        return codes, levels\n",
    "\n",
    "    def _dummy(self, codes, labels, prefix=\"_\"):\n",
    "        dummy_cols = [f\"{prefix}{label}\" for label in labels]\n",
    "\n",
    "        dummy_mat = np.eye(len(labels), dtype=np.uint8).take(codes, axis=0)\n",
    "        dummy_mat[codes == -1] = 0\n",
    "\n",
    "        # drop_first:\n",
    "        dummy_mat = dummy_mat[:, 1:]\n",
    "        dummy_cols = dummy_cols[1:]\n",
    "        return pd.DataFrame(dummy_mat, columns=dummy_cols)\n",
    "\n",
    "    def dummy_by_specified_labels(self, labels, code_labels, prefix=\"_\"):\n",
    "        codes = [self._get_index(labels, name) for name in code_labels]\n",
    "        return self._dummy(codes, labels, prefix)\n",
    "\n",
    "    def dummy_by_all_codes(self, codes, labels, prefix=\"_\"):\n",
    "        return self._dummy(codes, labels, prefix)\n",
    "\n",
    "\n",
    "class OpSaveCategoryColumns(OpDataFrame, CanSave):\n",
    "    def __init__(self, cat_cols, save_path, list_df_train = None):\n",
    "        OpDataFrame.__init__(self, op_name=\"OpSaveCategoryColumns\")\n",
    "        CanSave.__init__(self, save_path=save_path)\n",
    "        self.cat_cols = cat_cols\n",
    "        self.list_df_train = list_df_train\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        dummier = CategoryDummy()\n",
    "        d = {}\n",
    "\n",
    "        if self.list_df_train is None:\n",
    "            self.list_df_train = [_df]\n",
    "\n",
    "        _df_train = pd.concat(self.list_df_train, axis=0)\n",
    "\n",
    "        for cat_col in self.cat_cols:\n",
    "            d[cat_col] = dummier.codes_labels(_df_train[cat_col])\n",
    "\n",
    "        self.save(d)\n",
    "\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpLoadCategoryColumns(OpDataFrame, CanLoad):\n",
    "    def __init__(self, cat_cols, load_path, drop_orignal_col = True):\n",
    "        OpDataFrame.__init__(self, op_name=\"OpLoadCategoryColumns\")\n",
    "        CanLoad.__init__(self, load_path=load_path)\n",
    "\n",
    "        self.cat_cols = cat_cols\n",
    "        self.drop_orignal_col = drop_orignal_col\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        dummier = CategoryDummy()\n",
    "        d = self.load()\n",
    "\n",
    "        for cat_col in self.cat_cols:\n",
    "            _, labels = d[cat_col]\n",
    "\n",
    "            _df_col_cat = dummier.dummy_by_specified_labels(labels, _df[cat_col].values, prefix=f\"{cat_col}_\")\n",
    "            _df = pd.concat([_df.reset_index(drop=True), _df_col_cat.reset_index(drop=True)], axis=1)\n",
    "        if self.drop_orignal_col:\n",
    "            _df = _df.drop(labels=self.cat_cols, axis=1)\n",
    "\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpGroupbyExpander(OpDataFrame):\n",
    "    def __init__(self, group_fields, funcs=['mean', 'max', 'min']):\n",
    "        super(OpGroupbyExpander, self).__init__(op_name=\"OpGroupbyExpander\")\n",
    "        self.group_fields = group_fields\n",
    "        # self.compute_field = compute_field\n",
    "        self.funcs = funcs\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        compute_fields = _df.select_dtypes(include=['float', 'int']).columns\n",
    "\n",
    "        for compute_field in compute_fields:\n",
    "            _g_df = _df.groupby(by=self.group_fields).agg(\n",
    "                {compute_field: self.funcs}\n",
    "            )\n",
    "\n",
    "            _df = pd.merge(_df, _g_df, on=self.group_fields)\n",
    "\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpFeatureComposedExpander(OpDataFrame):\n",
    "    def __init__(self):\n",
    "        super(OpFeatureComposedExpander, self).__init__(op_name=\"OpFeatureComposedExpander\")\n",
    "        # self.num_cols = num_cols\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "\n",
    "        num_cols = _df.select_dtypes(include=['float', 'int', 'int64'])\n",
    "\n",
    "        for col_1 in num_cols:\n",
    "            for col_2 in num_cols:\n",
    "                _df[f\"{col_1}_and_{col_2}\"] = _df[col_1] * _df[col_2]\n",
    "\n",
    "        return _df\n",
    "\n",
    "\n",
    "class OpPipeLine(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Create pipeline of operation on the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, copy=True):\n",
    "        super(OpPipeLine, self).__init__(op_name=\"OpPipeLine\")\n",
    "        self.copy = copy\n",
    "        self.ops = []\n",
    "\n",
    "    def add_op(self, op):\n",
    "        self.ops.append(op)\n",
    "\n",
    "    def remove_op(self, removed_op_name):\n",
    "\n",
    "        self.ops = [x for x in self.ops if x.op_name != removed_op_name]\n",
    "\n",
    "    def replace_op(self, new_node_op):\n",
    "        for i, node_op in enumerate(self.ops):\n",
    "            if node_op.op_name == new_node_op.op_name and node_op.field == new_node_op.field:\n",
    "                self.ops[i] = new_node_op\n",
    "\n",
    "    def add_ops(self, ops):\n",
    "        self.ops.extend(ops)\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        if self.copy:\n",
    "            _df = _df.copy()\n",
    "\n",
    "        for node_op in self.ops:\n",
    "            try:\n",
    "                _df = node_op.op(_df)\n",
    "            except Exception as e:\n",
    "                if not node_op.ignore_if_failure:\n",
    "                    print(e)\n",
    "                    raise Exception(f\"Error when doing op: {node_op.get_info()}. Detail error: {e}\")\n",
    "                continue\n",
    "        return _df\n",
    "\n",
    "    def get_info(self):\n",
    "        return \" >> \".join([op.get_info() for op in self.ops])\n",
    "\n",
    "\n",
    "class OpMergeDataFrame(OpDataFrame):\n",
    "    \"\"\"\n",
    "    Merge 2 dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _df_main, left_on, right_on, how='inner'):\n",
    "        super(OpMergeDataFrame, self).__init__(op_name=\"OpMergeDataFrame\")\n",
    "        self._df_main = _df_main\n",
    "        self.how = how\n",
    "        self.left_on = left_on\n",
    "        self.right_on = right_on\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        return pd.merge(self._df_main, _df, how=self.how, left_on=self.left_on, right_on=self.right_on)\n",
    "\n",
    "\n",
    "class OpSelectNumericColumnOnly(OpDataFrame):\n",
    "    def __init__(self):\n",
    "        super(OpSelectNumericColumnOnly, self).__init__(op_name=\"OpSelectNumericColumnOnly\")\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        choose_cols = ['float', 'int', 'int64', 'bool', 'uint8']\n",
    "        return _df.select_dtypes(include=choose_cols)\n",
    "\n",
    "\n",
    "class OpAppliedFieldFunction(OpDataFrame):\n",
    "    def __init__(self, field, func, op_name, new_field=None):\n",
    "        OpDataFrame.__init__(self, op_name=op_name)\n",
    "        self.func = func\n",
    "        self.field = field\n",
    "        self.new_field = new_field\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        if self.new_field is None:\n",
    "            _df[self.field] = _df[self.field].apply(self.func)\n",
    "        else:\n",
    "            _df[self.new_field] = _df[self.field].apply(self.func)\n",
    "        return _df\n",
    "\n",
    "class OpFillNaWithValue(OpDataFrame):\n",
    "    def __init__(self, field, default_value):\n",
    "        OpDataFrame.__init__(self, \"OpFillNAWithValue\")\n",
    "        self.field = field\n",
    "        self.default_value = default_value\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].fillna(self.default_value)\n",
    "\n",
    "        return _df\n",
    "\n",
    "class OpFillNaWithMeanValue(OpDataFrame):\n",
    "    def __init__(self, field):\n",
    "        OpDataFrame.__init__(self, \"OpFillNaWithMeanValue\")\n",
    "        self.field = field\n",
    "\n",
    "\n",
    "    def op(self, _df: pd.DataFrame):\n",
    "        _df[self.field] = _df[self.field].fillna(_df[self.field].mean())\n",
    "\n",
    "        return _df\n",
    "\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "def convert_date_to_month_value(date_string):\n",
    "    if isinstance(date_string, (float, int)) and np.isnan(date_string):\n",
    "        return np.nan\n",
    "\n",
    "    date_value = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").date()\n",
    "\n",
    "    return (date(2018, 1, 1) - date_value).days\n",
    "\n",
    "\n",
    "def convert_year_to_value(value, current_year=2018):\n",
    "    if np.isnan(value) or value <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    if value > current_year:\n",
    "        return 0\n",
    "\n",
    "    return current_year - value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "convert_pipeline = OpPipeLine()\n",
    "\n",
    "# convert_pipeline.add_op(OpDropColumns(fields=['Unnamed: 0', 'LNR', 'CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP']))\n",
    "\n",
    "convert_pipeline.add_op(OpDropColumns(fields=['Unnamed: 0', 'LNR']))\n",
    "\n",
    "convert_pipeline.add_op(OpAppliedFieldFunction('EINGEFUEGT_AM', convert_date_to_month_value, 'convert_EINGEFUEGT_AM'))\n",
    "convert_pipeline.add_op(OpAppliedFieldFunction('EINGEZOGENAM_HH_JAHR', lambda c: convert_year_to_value(c, 2018),\n",
    "                                               'convert_EINGEZOGENAM_HH_JAHR'))\n",
    "convert_pipeline.add_op(\n",
    "    OpAppliedFieldFunction('GEBURTSJAHR', lambda c: convert_year_to_value(c, 2018), 'convert_GEBURTSJAHR'))\n",
    "convert_pipeline.add_op(\n",
    "    OpAppliedFieldFunction('AGER_TYP', lambda c: np.nan if c == -1 or c == 0 else c, 'convert_AGER_TYP'))\n",
    "\n",
    "# cat_cols = consider_category_columns(azdias, threshold=0.05)\n",
    "cat_cols = set(azdias.columns) - {'Unnamed: 0', 'LNR', 'EINGEFUEGT_AM', 'EINGEZOGENAM_HH_JAHR', 'GEBURTSJAHR', 'KBA13_ANZAHL_PKW'}\n",
    "\n",
    "# cat_cols = list(set(cat_cols) - {'EINGEFUEGT_AM', 'EINGEZOGENAM_HH_JAHR', 'GEBURTSJAHR'})\n",
    "\n",
    "cat_path = './cat_present.txt'\n",
    "\n",
    "convert_pipeline.add_op(OpSaveCategoryColumns(cat_cols=cat_cols, save_path=cat_path, list_df_train=[customers, azdias]))\n",
    "convert_pipeline.add_op(OpLoadCategoryColumns(cat_cols=cat_cols, load_path=cat_path))\n",
    "\n",
    "# convert_pipeline.add_op(OpAppliedFieldFunction('GEBURTSJAHR', bin_year, 'convert_BIN_YEAR', 'BIN_YEAR'))\n",
    "\n",
    "\n",
    "convert_pipeline.add_op(OpFillNaWithMeanValue('EINGEFUEGT_AM'))\n",
    "convert_pipeline.add_op(OpFillNaWithMeanValue('GEBURTSJAHR'))\n",
    "convert_pipeline.add_op(OpFillNaWithMeanValue('KBA13_ANZAHL_PKW'))\n",
    "convert_pipeline.add_op(OpFillNaWithMeanValue('EINGEZOGENAM_HH_JAHR'))\n",
    "\n",
    "\n",
    "formatted_customers = convert_pipeline.op(customers)\n",
    "formatted_azdias = convert_pipeline.op(azdias)\n",
    "\n",
    "# print(formatted_customers.head())\n",
    "#\n",
    "# print(formatted_azdias.head())\n",
    "\n",
    "\n",
    "print(f\"Shapes Azdias: {formatted_azdias.shape}, Customer: {formatted_customers.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def pca_decomposition(_df_train):\n",
    "    pca = PCA()\n",
    "    pca = pca.fit(_df_train)\n",
    "    return pca, pca.transform(_df_train)\n",
    "\n",
    "\n",
    "pca, Xt = pca_decomposition(formatted_azdias)\n",
    "\n",
    "Xt_cus = pca.transform(formatted_customers.drop(labels=['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], axis=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_2D():\n",
    "    plot = plt.scatter(Xt[:,0], Xt[:,1], c='yellow')\n",
    "    plt.scatter(Xt_cus[:,0], Xt_cus[:,1], c='green')\n",
    "    plt.legend(handles=plot.legend_elements()[0], labels=['General', 'Customer'])\n",
    "    plt.savefig(f'./images/visualize_2D.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_2D()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def visualize_3D():\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(Xt[:,0], Xt[:,1], Xt[:,2], color='yellow', alpha=0.3)\n",
    "    ax.scatter3D(Xt_cus[:,0], Xt_cus[:,1], Xt_cus[:,2], color='green', alpha=0.3)\n",
    "    plt.savefig(f'./images/visualize_3D.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_3D()\n",
    "\n",
    "# plot = plt.scatter(Xt[:,0], Xt[:,1], c='blue')\n",
    "# plt.scatter(Xt_cus[:,0], Xt_cus[:,1], c='yellow')\n",
    "# plt.legend(handles=plot.legend_elements()[0], labels=['General', 'Customer'])\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# formatted_customers['CUS'] = formatted_customers['CUSTOMER_GROUP'].apply(lambda x: 1 if x == \"MULTI_BUYER\" else 2)\n",
    "#\n",
    "# formatted_customers['CUS'].value_counts()\n",
    "#\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "#\n",
    "# # feature extraction\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "# rfe = RFE(model, n_features_to_select=30)\n",
    "# X = formatted_customers.drop(labels=['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], axis=1).values\n",
    "# y = formatted_customers['CUS'].to_numpy()\n",
    "# fit = rfe.fit(X, y)\n",
    "# print(\"Num Features: %d\" % fit.n_features_)\n",
    "# print(\"Selected Features: %s\" % fit.support_)\n",
    "# print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_dist_side_by_side(_df_general, _df_customers, col):\n",
    "    figure, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    # figure_name = plt.figure(figsize=(15, 8))\n",
    "\n",
    "    figure.set_figheight(5)\n",
    "    figure.set_figwidth(10)\n",
    "\n",
    "    ax1.set_title(f\"General: {col}\")\n",
    "    sns.distplot(_df_general[col], ax=ax1)\n",
    "\n",
    "    ax2.set_title(f\"Customer: {col}\")\n",
    "    sns.distplot(_df_customers[col], ax=ax2)\n",
    "\n",
    "    plt.savefig(f'./images/visualize_dist_side_by_side_{col}.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "\n",
    "visualize_dist_side_by_side(formatted_azdias, formatted_customers, 'GEBURTSJAHR')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def visualize_bar_size_by_side(_df_general, _df_customers, col):\n",
    "    figure, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    figure.set_figheight(5)\n",
    "    figure.set_figwidth(10)\n",
    "\n",
    "    ax1.set_title(f\"General: {col}\")\n",
    "    _df_general[col].value_counts().sort_index().plot(kind='bar', ax=ax1, color='steelblue')\n",
    "    # sns.barplot(_df_general.groupby(col)['LNR'].count(), ax=ax1)\n",
    "    #\n",
    "    ax2.set_title(f\"Customer: {col}\")\n",
    "    _df_customers[col].value_counts().sort_index().plot(kind='bar', ax=ax2, color='steelblue')\n",
    "\n",
    "    plt.savefig(f'./images/visualize_bar_size_by_side_{col}.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "\n",
    "visualize_bar_size_by_side(azdias, customers, 'AKT_DAT_KL')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_coff(_df):\n",
    "    f, ax = plt.subplots(figsize=(12, 8))\n",
    "    corr = _df.corr()\n",
    "    hm = sns.heatmap(round(corr, 2), annot=True, ax=ax, cmap=\"coolwarm\", fmt='.2f',\n",
    "                     linewidths=.05)\n",
    "    f.subplots_adjust(top=0.93)\n",
    "    t = f.suptitle('Attributes Correlation Heatmap', fontsize=14)\n",
    "\n",
    "\n",
    "visualize_coff(formatted_azdias)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# K-means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=10)\n",
    "Xt_kmeans = model.fit(Xt)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_categories(_Xt):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.scatter(_Xt[:,0], _Xt[:,1],  marker = 'o',\n",
    "                c=Xt_kmeans.labels_,\n",
    "                s=80, alpha=0.5)\n",
    "    # plt.scatter(centroids.iloc[:,0], centroids.iloc[:,1],\n",
    "    #             marker = 's', s=200, c=[0, 1, 2],\n",
    "    #             cmap = customcmap)\n",
    "    ax.set_xlabel(r'x', fontsize=14)\n",
    "    ax.set_ylabel(r'y', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.savefig(f'./images/visualize_categories.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_categories(Xt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict the categories into customers dataset\n",
    "\n",
    "x_cus_predict = model.predict(Xt_cus)\n",
    "\n",
    "unique, counts = np.unique(x_cus_predict, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def get_potential_cus(_Xt, categories, _df):\n",
    "    x_potential = model.predict(_Xt)\n",
    "    indies = np.argwhere(np.isin(x_potential, categories)).ravel()\n",
    "    return _df.iloc[indies]\n",
    "\n",
    "get_potential_cus(Xt, [0, 3, 9], azdias)\n",
    "\n",
    "get_potential_cus(Xt, [3], azdias)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('./data/mailout_train.csv')\n",
    "\n",
    "mailout_train.head()\n",
    "\n",
    "formated_mailout_train = convert_pipeline.op(mailout_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "formated_mailout_train['RESPONSE'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def xy_split(_df_train, drop_cols, label_cols):\n",
    "    X = _df_train.drop(labels=drop_cols, axis=1).values\n",
    "    y = _df_train[label_cols].values\n",
    "\n",
    "    if len(y.shape) == 2 and y.shape[1] == 1:\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, test_size=0.2):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def random_over_sampling(X, y):\n",
    "    sampler = RandomOverSampler()\n",
    "    return sampler.fit_resample(X, y)\n",
    "\n",
    "\n",
    "X, y = xy_split(formated_mailout_train, ['RESPONSE'], ['RESPONSE'])\n",
    "X, y = random_over_sampling(X, y)\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      8461\n",
      "           1       0.98      0.92      0.95      8511\n",
      "\n",
      "    accuracy                           0.95     16972\n",
      "   macro avg       0.95      0.95      0.95     16972\n",
      "weighted avg       0.95      0.95      0.95     16972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST CLASSIFICATION\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "report = classification_report(y_test,y_pred)\n",
    "\n",
    "print(report)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\hotronghai\\onedrive\\python\\ds_nano_webdev\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "d:\\hotronghai\\onedrive\\python\\ds_nano_webdev\\venv\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREDICT TEST DATA\n",
    "\n",
    "mailout_test = pd.read_csv('./data/mailout_test.csv')\n",
    "formated_mailout_test = convert_pipeline.op(mailout_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\hotronghai\\onedrive\\python\\ds_nano_webdev\\venv\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 893 customers (which response is 1) in 42833 records\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = clf.predict(formated_mailout_test)\n",
    "\n",
    "print(f\"There are: {len(y_test_pred[y_test_pred == 1])} customers (which response is 1) in {mailout_test.shape[0]} records\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "41940"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test1[test1 == 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}